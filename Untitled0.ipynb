{
 "metadata": {
  "name": "",
  "signature": "sha256:e3805484860f166b3b34ce2ec5b926b18a10c6d9250e5d5bdeaf2ac9761392a3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas\n",
      "import os\n",
      "\n",
      "train_data_dir = 'mails_bodies_train'\n",
      "test_data_dir = 'mails_bodies_test'\n",
      "class_lables_file = 'spam-mail.tr.label'\n",
      "\n",
      "# Table of file number and corresponding class lable (0 - not spam, 1 - spam)\n",
      "class_lables = pandas.read_csv(class_lables_file)\n",
      "\n",
      "train_files_names = os.listdir(train_data_dir)\n",
      "test_files_names = os.listdir(test_data_dir)\n",
      "# Create table with columns text and class to fill in\n",
      "train_data = pandas.DataFrame({'text': [], 'class': []})\n",
      "test_data = pandas.DataFrame({'Id': [], 'text': []})\n",
      "\n",
      "for file_name in train_files_names:\n",
      "    \n",
      "    srcpath = os.path.join(train_data_dir, file_name)\n",
      "    # Fetch file's id from file's name\n",
      "    file_number_string = file_name.split('_')[1].split('.')[0]\n",
      "    file_number = int(file_number_string)\n",
      "    # Get the class of training sample by id\n",
      "    class_number = class_lables[class_lables['Id'] == file_number]['Prediction'].iloc[0]\n",
      "    # All files were translated to utf-8\n",
      "    text = unicode( open(srcpath).read(), encoding='utf-8')\n",
      "    data_frame = pandas.DataFrame({'text': [text], 'class': [class_number]})\n",
      "    train_data = train_data.append(data_frame, ignore_index=True)\n",
      "\n",
      "for file_name in test_files_names:\n",
      "    \n",
      "    srcpath = os.path.join(test_data_dir, file_name)\n",
      "    # Fetch file's id from file's name\n",
      "    file_number_string = file_name.split('_')[1].split('.')[0]\n",
      "    file_number = int(file_number_string)\n",
      "    # All files were translated to utf-8\n",
      "    text = unicode( open(srcpath).read(), encoding='utf-8')\n",
      "    data_frame = pandas.DataFrame({'Id': [file_number], 'text': [text]})\n",
      "    test_data = test_data.append(data_frame, ignore_index=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.base as base\n",
      "import pickle\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "import re\n",
      "from sklearn.pipeline import Pipeline, FeatureUnion\n",
      "import numpy\n",
      "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
      "from sklearn.svm import SVC\n",
      "from scipy.sparse import csr_matrix\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "\n",
      "contains_url_regex = re.compile(r\"http[s]?://\")\n",
      "contains_html_regex = re.compile(r\"<[^>]+>\")\n",
      "contains_spam_regex = re.compile(r\"spam\", re.IGNORECASE)\n",
      "response_email_regex = re.compile(r\"Re:\", re.IGNORECASE)\n",
      "\n",
      "\n",
      "class Contains_URL_Transformer(BaseEstimator, TransformerMixin):\n",
      "    \n",
      "    def __init__(self):\n",
      "        pass\n",
      "    \n",
      "    def transform(self, X):\n",
      "        \n",
      "        column_amount, = X.shape\n",
      "        result = csr_matrix((column_amount, 1))\n",
      "        \n",
      "        for index, doc in enumerate(X):\n",
      "            contains_url = contains_url_regex.search(doc)\n",
      "            if contains_url is not None:\n",
      "                result[index, 0] = 1\n",
      "        \n",
      "        return result\n",
      "\n",
      "    def fit(self, X, y=None):\n",
      "        return self\n",
      "\n",
      "class Contains_Spam_Transformer(BaseEstimator, TransformerMixin):\n",
      "    \n",
      "    def __init__(self):\n",
      "        pass\n",
      "    \n",
      "    def transform(self, X):\n",
      "        \n",
      "        column_amount, = X.shape\n",
      "        result = csr_matrix((column_amount, 1))\n",
      "        \n",
      "        for index, doc in enumerate(X):\n",
      "            contains_url = contains_spam_regex.search(doc)\n",
      "            if contains_url is not None:\n",
      "                result[index, 0] = 1\n",
      "        \n",
      "        return result\n",
      "\n",
      "    def fit(self, X, y=None):\n",
      "        return self\n",
      "\n",
      "class Contains_HTML_Transformer(BaseEstimator, TransformerMixin):\n",
      "    \n",
      "    def __init__(self):\n",
      "        pass\n",
      "    \n",
      "    def transform(self, X):\n",
      "        \n",
      "        column_amount, = X.shape\n",
      "        result = csr_matrix((column_amount, 1))\n",
      "        \n",
      "        for index, doc in enumerate(X):\n",
      "            contains_url = contains_html_regex.search(doc)\n",
      "            if contains_url is not None:\n",
      "                result[index, 0] = 1\n",
      "        \n",
      "        return result\n",
      "\n",
      "    def fit(self, X, y=None):\n",
      "        return self\n",
      "\n",
      "class Contains_Re_Transformer(BaseEstimator, TransformerMixin):\n",
      "    \n",
      "    def __init__(self):\n",
      "        pass\n",
      "    \n",
      "    def transform(self, X):\n",
      "        \n",
      "        column_amount, = X.shape\n",
      "        result = csr_matrix((column_amount, 1))\n",
      "        \n",
      "        for index, doc in enumerate(X):\n",
      "            contains_url = response_email_regex.search(doc)\n",
      "            if contains_url is not None:\n",
      "                result[index, 0] = 1\n",
      "        \n",
      "        return result\n",
      "\n",
      "    def fit(self, X, y=None):\n",
      "        return self\n",
      "\n",
      "class Text_Length_Transformer(BaseEstimator, TransformerMixin):\n",
      "    \n",
      "    def __init__(self):\n",
      "        pass\n",
      "    \n",
      "    def transform(self, X):\n",
      "        \n",
      "        column_amount, = X.shape\n",
      "        result = numpy.ndarray((column_amount, 1))\n",
      "        \n",
      "        for index, doc in enumerate(X):\n",
      "            result[index, 0] = len(doc)\n",
      "        \n",
      "        return result\n",
      "\n",
      "    def fit(self, X, y=None):\n",
      "        return self\n",
      "\n",
      "\n",
      "pipeline = Pipeline([\n",
      "    ('features', FeatureUnion([\n",
      "        ('words', Pipeline([\n",
      "            ('vect',  CountVectorizer()),\n",
      "            ('frequency_transform',  TfidfTransformer())\n",
      "        ])),\n",
      "        ('url_feature',  Contains_URL_Transformer()),\n",
      "        ('html_feature', Contains_HTML_Transformer()),\n",
      "\n",
      "        ('scaled_length_feature', Pipeline([\n",
      "            ('length_feature', Text_Length_Transformer()),\n",
      "            ('frequency_transform',  StandardScaler())\n",
      "        ])),\n",
      "            \n",
      "        ('spam_feature', Contains_Spam_Transformer()),\n",
      "        ('response_feature', Contains_Re_Transformer())\n",
      "        ])),\n",
      "    ('clf',  SVC())\n",
      "])\n",
      "\n",
      "parameters = {\n",
      "    'features__words__vect__min_df': (1, 3, 5),\n",
      "    'features__words__vect__token_pattern': (r\"\\b[^\\W\\d_]+\\b\",),\n",
      "    'features__words__vect__binary': (False,),\n",
      "    'features__words__frequency_transform__use_idf' : (True,),\n",
      "    #'vect__max_features': (None, 5000, 10000, 50000),\n",
      "    'features__words__vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
      "    'clf__C': (1, 5, 10),\n",
      "    'clf__kernel': ('linear', 'rbf')\n",
      "    #'tfidf__use_idf': (True, False)\n",
      "    #'tfidf__norm': ('l1', 'l2'),\n",
      "    #'clf__alpha': (0.00001, 0.000001),\n",
      "    #'clf__penalty': ('l2', 'elasticnet'),\n",
      "    #'clf__n_iter': (10, 50, 80),\n",
      "}\n",
      "\n",
      "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
      "\n",
      "data_column = numpy.asarray(train_data['text'])\n",
      "\n",
      "grid_search.fit(data_column, numpy.asarray(train_data['class']))\n",
      "\n",
      "best_parameters = grid_search.best_estimator_.get_params()\n",
      "\n",
      "for param_name in sorted(parameters.keys()):\n",
      "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
      "\n",
      "best_params_dict = grid_search.best_params_\n",
      "\n",
      "with open('best_estimator_params.pkl', 'w') as f:\n",
      "    pickle.dump(best_params_dict, f)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    8.0s\n",
        "[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:  2.7min\n",
        "[Parallel(n_jobs=-1)]: Done 102 out of 108 | elapsed:  5.8min remaining:   20.5s\n",
        "[Parallel(n_jobs=-1)]: Done 108 out of 108 | elapsed:  6.1min finished\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\tclf__C: 5\n",
        "\tclf__kernel: 'linear'\n",
        "\tfeatures__words__frequency_transform__use_idf: True\n",
        "\tfeatures__words__vect__binary: False\n",
        "\tfeatures__words__vect__min_df: 5\n",
        "\tfeatures__words__vect__ngram_range: (1, 2)\n",
        "\tfeatures__words__vect__token_pattern: '\\\\b[^\\\\W\\\\d_]+\\\\b'\n"
       ]
      }
     ],
     "prompt_number": 90
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grid_search.best_score_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 91,
       "text": [
        "0.97640000000000005"
       ]
      }
     ],
     "prompt_number": 91
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grid_search.best_params_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 84,
       "text": [
        "{'clf__C': 10,\n",
        " 'clf__kernel': 'linear',\n",
        " 'features__words__frequency_transform__use_idf': True,\n",
        " 'features__words__vect__binary': False,\n",
        " 'features__words__vect__min_df': 5,\n",
        " 'features__words__vect__ngram_range': (1, 2),\n",
        " 'features__words__vect__token_pattern': '\\\\b[^\\\\W\\\\d_]+\\\\b'}"
       ]
      }
     ],
     "prompt_number": 84
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "st = \"boo boo boo boo </i>\"\n",
      "result = re.search(r\"<[^>]+>\", st)\n",
      "\n",
      "if result is not None:\n",
      "    print 'contains'\n",
      "else:\n",
      "    print 'none'\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "contains\n"
       ]
      }
     ],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# With custom feature(contains URL + text length + html_tags)\n",
      "best_result_so_far = 0.9788"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "\n",
      "best_params_dict = grid_search.best_params_\n",
      "\n",
      "with open('best_estimator_params.pkl', 'w') as f:\n",
      "    pickle.dump(best_params_dict, f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('best_estimator_params.pkl', 'r') as f:\n",
      "    data = pickle.load(f)\n",
      "\n",
      "pipeline.set_params(**data)\n",
      "\n",
      "estimator = grid_search.best_estimator_\n",
      "result = estimator.predict(numpy.asarray(test_data['text']))\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "result\n",
      "\n",
      "final = pandas.DataFrame({'Id': numpy.asarray(test_data['Id']), 'Prediction': numpy.asarray(result)})\n",
      "\n",
      "final = final.astype(int)\n",
      "\n",
      "final = final.sort(['Id'])\n",
      "\n",
      "#ones = (final['Prediction'] == 1)\n",
      "#zeros = (final['Prediction'] == 0)\n",
      "#final.ix[ones, 'Prediction'] = 0\n",
      "#final.ix[zeros, 'Prediction'] = 1\n",
      "\n",
      "\n",
      "final.to_csv('final.csv', index=False)\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 74
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_data = pandas.DataFrame({'text': []})\n",
      "\n",
      "dummy_1 = pandas.DataFrame({'text': ['bla bla']}, index=[2])\n",
      "dummy_2 = pandas.DataFrame({'text': ['bla']}, index=[0])\n",
      "\n",
      "test_data = test_data.append(dummy_1)\n",
      "test_data = test_data.append(dummy_2)\n",
      "\n",
      "tmp = test_data['text'].sort_index()\n",
      "\n",
      "numpy.asarray(tmp)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "array(['bla', 'bla bla'], dtype=object)"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_data.to_csv('dummy.csv', index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pandas.read_csv('dummy.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>text</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> bla bla</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>     bla</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 29,
       "text": [
        "      text\n",
        "0  bla bla\n",
        "1      bla"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class_lables[class_lables['Id'] == file_number]['Prediction'].iloc[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 102,
       "text": [
        "1"
       ]
      }
     ],
     "prompt_number": 102
    }
   ],
   "metadata": {}
  }
 ]
}