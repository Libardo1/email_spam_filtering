{
 "metadata": {
  "name": "",
  "signature": "sha256:df9604f67fdce81334ede5fc49ce2a6a625512353a799fa75d7be74f16274afc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Classifier Training part"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this part we do a grid search of hyperparameters for our svm classifier and check each\n",
      "combination via 3-fold cross validation. On the output we get the best classifier and its\n",
      "score that it got on cross validation.\n",
      "\n",
      "The parameters that are tested can be seen in $\\textbf{spam_filter.py}$ file.\n",
      "\n",
      "The training set is loaded from the $\\textbf{spam_data.py}$ module.\n",
      "\n",
      "Basically, for training only mails' bodies are extracted from the raw mail files.\n",
      "\n",
      "After the training the trained classifier is saved to a specified in $\\textbf{file_name_to_save_classifier_to}$ variable file.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import spam_filter\n",
      "\n",
      "file_name_to_save_classifier_to = 'classifier_dump/classifier.dump'\n",
      "\n",
      "classifier = spam_filter.get_best_classifier_using_grid_search()\n",
      "\n",
      "spam_filter.save_classifier_into_file(classifier, file_name_to_save_classifier_to)\n",
      "\n",
      "print classifier['classifier_score']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:   18.3s\n",
        "[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:  2.8min\n",
        "[Parallel(n_jobs=-1)]: Done 102 out of 108 | elapsed:  5.7min remaining:   20.2s\n",
        "[Parallel(n_jobs=-1)]: Done 108 out of 108 | elapsed:  6.1min finished\n",
        "/home/dan/anaconda/lib/python2.7/site-packages/scipy/sparse/compressed.py:690: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
        "  SparseEfficiencyWarning)\n",
        "/home/dan/anaconda/lib/python2.7/site-packages/scipy/sparse/compressed.py:690: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
        "  SparseEfficiencyWarning)\n",
        "/home/dan/anaconda/lib/python2.7/site-packages/scipy/sparse/compressed.py:690: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
        "  SparseEfficiencyWarning)\n",
        "/home/dan/anaconda/lib/python2.7/site-packages/scipy/sparse/compressed.py:690: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
        "  SparseEfficiencyWarning)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.9764\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/home/dan/anaconda/lib/python2.7/site-packages/scipy/sparse/compressed.py:690: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
        "  SparseEfficiencyWarning)\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Test part for Kaggle competition"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This part of code loads a previously trained classifier and creates a predictions for the\n",
      "test data, that can be later loaded into kaggle check system."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy\n",
      "import pandas\n",
      "import spam_filter\n",
      "import spam_data\n",
      "\n",
      "classifier = spam_filter.load_model_from_file('classifier_dump/classifier.dump')\n",
      "\n",
      "classifier_accuracy = classifier['classifier_score']\n",
      "\n",
      "classifier_obj = classifier['classifier']\n",
      "\n",
      "test_data = spam_data.fetch_test_set()\n",
      "\n",
      "result = classifier_obj.predict(numpy.asarray(test_data['text']))\n",
      "\n",
      "final = pandas.DataFrame({'Id': numpy.asarray(test_data['Id']), 'Prediction': numpy.asarray(result)})\n",
      "\n",
      "final = final.astype(int)\n",
      "\n",
      "final = final.sort(['Id'])\n",
      "\n",
      "final.to_csv('final_1.csv', index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = classifier['classifier']\n",
      "test.get_params()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "{'clf': SVC(C=5, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
        "   kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
        "   shrinking=True, tol=0.001, verbose=False),\n",
        " 'clf__C': 5,\n",
        " 'clf__cache_size': 200,\n",
        " 'clf__class_weight': None,\n",
        " 'clf__coef0': 0.0,\n",
        " 'clf__degree': 3,\n",
        " 'clf__gamma': 0.0,\n",
        " 'clf__kernel': 'linear',\n",
        " 'clf__max_iter': -1,\n",
        " 'clf__probability': False,\n",
        " 'clf__random_state': None,\n",
        " 'clf__shrinking': True,\n",
        " 'clf__tol': 0.001,\n",
        " 'clf__verbose': False,\n",
        " 'features': FeatureUnion(n_jobs=1,\n",
        "        transformer_list=[('words', Pipeline(steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, charset=None,\n",
        "         charset_error=None, decode_error=u'strict',\n",
        "         dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
        "         lowercase=True, max_df=1.0, max_features=None, min_d...])), ('spam_feature', Contains_Spam_Transformer()), ('response_feature', Contains_Re_Transformer())],\n",
        "        transformer_weights=None),\n",
        " 'features__html_feature': Contains_HTML_Transformer(),\n",
        " 'features__response_feature': Contains_Re_Transformer(),\n",
        " 'features__scaled_length_feature': Pipeline(steps=[('length_feature', Text_Length_Transformer()), ('frequency_transform', StandardScaler(copy=True, with_mean=True, with_std=True))]),\n",
        " 'features__scaled_length_feature__frequency_transform': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
        " 'features__scaled_length_feature__frequency_transform__copy': True,\n",
        " 'features__scaled_length_feature__frequency_transform__with_mean': True,\n",
        " 'features__scaled_length_feature__frequency_transform__with_std': True,\n",
        " 'features__scaled_length_feature__length_feature': Text_Length_Transformer(),\n",
        " 'features__spam_feature': Contains_Spam_Transformer(),\n",
        " 'features__url_feature': Contains_URL_Transformer(),\n",
        " 'features__words': Pipeline(steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, charset=None,\n",
        "         charset_error=None, decode_error=u'strict',\n",
        "         dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
        "         lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
        "         ngram_range=(1, 2), prep...ansform', TfidfTransformer(norm=u'l2', smooth_idf=True, sublinear_tf=False,\n",
        "          use_idf=True))]),\n",
        " 'features__words__frequency_transform': TfidfTransformer(norm=u'l2', smooth_idf=True, sublinear_tf=False,\n",
        "          use_idf=True),\n",
        " 'features__words__frequency_transform__norm': u'l2',\n",
        " 'features__words__frequency_transform__smooth_idf': True,\n",
        " 'features__words__frequency_transform__sublinear_tf': False,\n",
        " 'features__words__frequency_transform__use_idf': True,\n",
        " 'features__words__vect': CountVectorizer(analyzer=u'word', binary=False, charset=None,\n",
        "         charset_error=None, decode_error=u'strict',\n",
        "         dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
        "         lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
        "         ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
        "         strip_accents=None, token_pattern='\\\\b[^\\\\W\\\\d_]+\\\\b',\n",
        "         tokenizer=None, vocabulary=None),\n",
        " 'features__words__vect__analyzer': u'word',\n",
        " 'features__words__vect__binary': False,\n",
        " 'features__words__vect__charset': None,\n",
        " 'features__words__vect__charset_error': None,\n",
        " 'features__words__vect__decode_error': u'strict',\n",
        " 'features__words__vect__dtype': numpy.int64,\n",
        " 'features__words__vect__encoding': u'utf-8',\n",
        " 'features__words__vect__input': u'content',\n",
        " 'features__words__vect__lowercase': True,\n",
        " 'features__words__vect__max_df': 1.0,\n",
        " 'features__words__vect__max_features': None,\n",
        " 'features__words__vect__min_df': 5,\n",
        " 'features__words__vect__ngram_range': (1, 2),\n",
        " 'features__words__vect__preprocessor': None,\n",
        " 'features__words__vect__stop_words': None,\n",
        " 'features__words__vect__strip_accents': None,\n",
        " 'features__words__vect__token_pattern': '\\\\b[^\\\\W\\\\d_]+\\\\b',\n",
        " 'features__words__vect__tokenizer': None,\n",
        " 'features__words__vect__vocabulary': None}"
       ]
      }
     ],
     "prompt_number": 5
    }
   ],
   "metadata": {}
  }
 ]
}